##### 为新转录本的gtf文件添加预测的CDS信息

```python
# 来源于penglingwei的拷贝
import os
import pandas as pd
import numpy as np

# lwpeng
# wd = "/public/home/lwpeng/pan_transcriptome/gtf_transform"
# gtf = f"{wd}/ref_gtf/hereford.gtf"
# anno_gtf = f"{wd}/ref_gtf/Bos_taurus.ARS-UCD1.2.104.gtf"
# out_folder = f"{wd}/ref_gtf/generate_gtf" ; os.system(f"mkdir -p {out_folder}")
# pep = f"{wd}/ref_gtf/transdecoder/pep_header.txt"

# penglingwei
wd = "/home/penglingwei/pan_transcriptome/pan_process/gtf_transform"
gtf = "/home/penglingwei/pan_transcriptome/ngs_process/hereford.gtf"
anno_gtf = "/home/penglingwei/personal_project/ref/Bos_taurus.ARS-UCD1.2.104.gtf"
# out_folder = f"{wd}/ref_gtf/generate_gtf" ; os.system(f"mkdir -p {out_folder}")
pep = f"{wd}/predict_transcript/pep_header.txt"
global out_gtf ; out_gtf = f"{wd}/hereford.add_CDS.gtf" ; os.system(f"> {out_gtf}")


# 解析gtf文件
def parseGtf(gtf):
    df = pd.read_table(gtf, header=None, sep="\t", comment="#", low_memory=False)
    df.columns = ["chromosome", "source", "type", "start", "end", "score", "strand", "phase", "attribute"]
    df = df[df["type"].isin(["transcript", "exon", "CDS"])]
    function = lambda attribute: [x.split("\"")[1] for x in attribute.split(";") if "transcript_id" in x][0] 
    df["transcript"] = df["attribute"].apply(function)
    return df

# 注释gtf中提取CDS
def extract_chromosome_CDS(transcript, cds_df):
    df = cds_df.loc[cds_df["transcript"] == transcript, :].copy()
    strand = df["strand"].iloc[0]
    if strand == "+":
        start, end = df["start"].iloc[0], df["end"].iloc[-1]
    else:
        start, end = df["end"].iloc[0], df["start"].iloc[-1]
    return [start, end]

# TransDecoder结果筛选预测的CDS
def extract_transcript_CDS(pep):
    df = pd.read_table(pep, header=None, sep=" ", usecols=[4, 6, 7])
    df = pd.DataFrame({
        "type": df.loc[:, 4].str.split(":", expand=True)[1],
        "strand": df.loc[:, 6].str.split(",", expand=True)[0],
        "score": df.loc[:, 6].str.split("=", expand=True)[1].astype("float"),
        "transcript": df.loc[:, 7].str.split(":", expand=True)[0],
        "start_end": df.loc[:, 7].str.split(":|\(", expand=True)[1]
        })

    df = df[df["transcript"].str.contains("MSTRG")]
    df["start"] = df["start_end"].str.split("-", expand=True)[0].astype("int")
    df["end"] = df["start_end"].str.split("-", expand=True)[1].astype("int")

    df = df[(df["type"]=="complete") & (df["strand"]=="(+)")] 
    index = df.groupby("transcript")["score"].agg(lambda scores: scores.idxmax())
    df = df.loc[index, :]

    return df.loc[:, ["transcript", "start", "end"]]

# 转录本水平坐标转换为染色体水平坐标
def site_transcript2chromosome(exon_df, transcript, site):
    df = exon_df.reset_index(drop=True)
    strand = df["strand"].iloc[0]
    exon_lengths = df["end"] - df["start"] + 1
    iteration_lengths = np.cumsum(exon_lengths.values) if strand == "+" else np.cumsum(exon_lengths.values[::-1])[::-1]
    diffs = iteration_lengths - site
    min = diffs[diffs >= 0].min()
    i = np.where(diffs == min)[0][0]
    site = df.loc[i, "end"] - min if strand == "+" else df.loc[i, "start"] + min
    return site

# gtf_df中增加CDS坐标描述列
def add_CDS(transcript, gtf_df, annotated_CDS_df, annotated_transcripts, predicted_CDS_df, predicted_transcripts):
    transcript_df = gtf_df.loc[(gtf_df["transcript"]==transcript) & (gtf_df["type"].isin(["transcript", "exon"]))].reset_index(drop=True)
    transcript_df["CDS_start"], transcript_df["CDS_end"], transcript_df["CDS_type"] = "-", "-", "-"
    #1 获取注释和新转录本的CDS区域
    if transcript in annotated_transcripts:
        start, end = extract_chromosome_CDS(transcript, annotated_CDS_df)
    elif transcript in predicted_transcripts:
        exon_df = transcript_df[transcript_df["type"]=="exon"]
        start, end = predicted_CDS_df.loc[transcript, "start"], predicted_CDS_df.loc[transcript, "end"]
        start = site_transcript2chromosome(exon_df, transcript, start)
        end = site_transcript2chromosome(exon_df, transcript, end)
    else:
        # print(transcript_df.loc[:, ["start", "end", "CDS_type", "CDS_start", "CDS_end"]])
        return transcript_df

    def locate_CDS(starts, start_end):
        diffs = start_end - starts
        min = diffs[diffs >= 0].min()
        diffs = diffs.iloc[1: ]
        return diffs[diffs==min].index[0]

    #1 定位CDS端点所在的exon(locate_CDS)
    starts = transcript_df["start"]
    strand = transcript_df["strand"].iloc[0]
    start, end = start if strand == "+" else end, end if strand == "+" else start  # 转录本start/end转换为StringTie gtf的start/end
    start_index = locate_CDS(starts, start) 
    end_index = locate_CDS(starts, end) 


    #2 定义CDS端点的坐标
    transcript_df.loc[start_index, "CDS_start"] = start
    transcript_df.loc[end_index, "CDS_end"] = end

    #3 定义CDS端点之间的坐标; 定义CDS对应的type
    start2end_indexes = list(range(start_index, end_index + 1))
    transcript_df.loc[start2end_indexes[1 : ], "CDS_start"] = transcript_df.loc[start2end_indexes[1: ], "start"]
    transcript_df.loc[start2end_indexes[ : -1], "CDS_end"] = transcript_df.loc[start2end_indexes[ : -1], "end"]
    transcript_df.loc[start2end_indexes, "CDS_type"] = "CDS"
    return transcript_df

import re


def single_execute(transcript, gtf_df, annotated_CDS_df, annotated_transcripts, predicted_CDS_df, predicted_transcripts): # 需要共享变量输入
    # 在含transcript和exon的gtf_df文件中添加新列CDS_type、CDS_start和CDS_end
    df = add_CDS(transcript, gtf_df, annotated_CDS_df, annotated_transcripts, predicted_CDS_df, predicted_transcripts)
    df = df.astype(str)
    # 根据gtf_df生成gtf文件行
    def generate_gtf(row, df): 
        columns = ["chromosome", "source", "type", "start", "end", "score", "strand", "phase", "attribute"]
        CDS_columns = ["chromosome", "source", "CDS_type", "CDS_start", "CDS_end", "score", "strand", "phase", "attribute"]
        CDS_type = df.loc[row, "CDS_type"]

        line = df.loc[row, columns].str.cat(sep="\t")
        if CDS_type == "-":
            return line
        else:
            return line + "\n" + re.sub(r' exon_number "[\d+]";', '', df.loc[row, CDS_columns].str.cat(sep="\t"))

    # 逐行处理并生成文本
    return pd.Series(df.index).apply(lambda row: generate_gtf(row, df)).str.cat(sep="\n")

def write_line(line):
    global lines 
    if lines == "":
        lines = line
    else:
        lines = lines + "\n" + line

#1 获取已知转录本CDS区域在染色体上的坐标
annotated_CDS_df = parseGtf(anno_gtf)
annotated_CDS_df = annotated_CDS_df.loc[annotated_CDS_df["type"]=="CDS", ["transcript", "start", "end", "strand"]]
annotated_transcripts = annotated_CDS_df["transcript"].drop_duplicates().tolist()

#2 获取新转录本预测的CDS区域在染色体上的坐标(基于Transdecoder)
predicted_CDS_df = extract_transcript_CDS(pep).set_index("transcript", drop=True)
predicted_transcripts = predicted_CDS_df.index.tolist()

import multiprocessing

#3 多进程运行
gtf_df = parseGtf(gtf)
transcripts = gtf_df["transcript"].drop_duplicates()

# 创建共享变量
manager = multiprocessing.Manager()
s_v = share_variable = manager.list()
share_variable.append(gtf_df)
share_variable.append(annotated_CDS_df) ; share_variable.append(annotated_transcripts)
share_variable.append(predicted_CDS_df) ; share_variable.append(predicted_transcripts) 

from more_itertools import chunked
pool = multiprocessing.Pool(processes=5)
for transcript_group in chunked(transcripts, 100):
    global lines ; lines = ""
    for transcript in transcript_group:
        # a=single_execute(transcript, gtf_df, annotated_CDS_df, annotated_transcripts, predicted_CDS_df, predicted_transcripts)
        pool.apply_async(single_execute, (transcript, s_v[0], s_v[1], s_v[2], s_v[3], s_v[4]), callback=write_line)
    with open(out_gtf, "a") as f:
        f.write(f"{lines}\n")
pool.close()
pool.join()

```









##### conda

```sh
conda info --envs
conda env remove --prefix <environment_path>
conda env remove --name <env_name>
conda create --name myenv r-base=4.1.1
conda create --name R4.1.1 --clone myenv # 克隆环境
conda env export --file myenv.yaml --name myenv # 导出环境
conda env create -f myenv.yaml # 部署新环境
```



##### 获得物种谱系

```
taxid=`echo $1 | taxonkit name2taxid | cut -f 2`
lineage=`echo ${taxid} | taxonkit lineage | taxonkit reformat | cut -f 3`
Phylum=`echo ${lineage} | cut -d ";" -f 2`
if [[ "${Phylum}" = "Chordata" ]] ; then
  echo ${lineage} > ${taxid}.txt
fi
```



##### aspera优先的原始fastq数据下载

```
# input
experiment_accession=$1 ; wd=raw
keyfile=/share/home/yzwl_zhouy/software/aspera-3.7.4/etc/asperaweb_id_dsa.openssh

# output
outfolder=${wd}/${experiment_accession} ; mkdir -p ${outfolder}
tsv=${outfolder}/${experiment_accession}.tsv
url="https://www.ebi.ac.uk/ena/portal/api/filereport?accession=${experiment_accession}&result=read_run&fields=run_accession,fastq_md5,fastq_aspera&format=tsv&download=true&limit=0"

echo "=>=>=>${experiment_accession}"
date

echo ">>>step1：从ebi中下载${experiment_accession}的详细信息" 
wget -c ${url} -O ${tsv} 
if [ -s "${tsv}" ] ; then 
  status=1 
else
  echo ">>>无法获得ebi中${experiment_accession}的样本信息" 
  status=0
fi

echo ">>>step2：检测fastq_aspera是否有效" 
if [ "${status}" == 1 ] ; then
  read -r run fastq_md5 fastq_aspera <<< `awk 'NR==2' ${tsv}`
  num=0
  if [ "${fastq_aspera}" == "" ] ; then echo ">>>fastq_aspera无效" ; status=0 ; fi
fi

if [ "${status}" == 1 ] ; then
  fq=${outfolder}/${run}.fastq.gz
  echo ">>>step3：aspera开始下载数据" 
  while true
  do
    num=$((num+1))
    if [ ${num} -gt 5 ] ; then echo ">>>${run}下载失败, 已多次尝试" ; status=0 ; break ; fi

    if [ ! -f ${fq} ] || [ `md5sum ${fq} | awk '{print $1}'` != "${fastq_md5}" ] ; then
      echo ">>>第${num}次下载" 
      ascp -vQT -l 500M -P33001 -k 1 -i ${keyfile} era-fasp@${fastq_aspera} ${fq}
    else
      echo ">>>ascp成功下载${experiment_accession}数据, 第${num}次成功"
      break
    fi
  done
fi

if [ "${status}" == 0 ] ; then
  echo ">>>step2：prefetch开始下载数据" 
  prefetch ${experiment_accession} -O ${outfolder} && fastq-dump `ls ${outfolder}/*/*sra` -O ${outfolder} && echo ">>>prefetch成功下载${experiment_accession}数据"
fi
date
```



##### 解决终端自动退出问题

```
ServerAliveInterval 60 每隔一段时间与ssh服务器通信一次
```

##### linux消除文件中的^M

vim命令界面输入：

```
%s/\r//g
```



##### 新知识

直接调用包函数

```R
biomaRt::getBM()
```



##### parallel包实现R语言多线程

```R
# 检测核数
detectCores()
# 指定核数
cl <- makeCluster(8)
# 传入外部变量、函数等(R包需要内部加载)
clusterExport(cl, c("df", "function"))
# 准备用于多线程函数
f <- function(x) {
    return(x)
}
# parLapply执行多线程
parLapply(cl, c(x1, x2, x3), f)
# 释放核 
stopCluster(cl)              
```



##### 小提琴图·

```R
library(ggplot2)
p <- ggplot(df, aes(x=gene, y=tpm, fill=population)) + 
        geom_boxplot(aes(fill=population), linewidth=0.1, outlier.color="white")
    p <- p + ylab("log10 (TPM)")
    p <- p + scale_fill_manual(
        values = c(Beef="#1F77B4", Dairy="#2CA02C", Africa="#FF7F0E", China="#D62728"),
        breaks = c("Beef", "Dairy", "China", "Africa"))
    p <- p + scale_y_continuous(expand=c(0, 0))
    p <- p + theme_bw() + theme(panel.border=element_blank(), panel.grid.major=element_blank(), panel.grid.minor=element_blank(), axis.line=element_line(colour="black"))
    p <- p + theme(axis.title.x = element_blank(), axis.text.x=element_text(size=12, color="black"), axis.text.y=element_text(size=12, color="black"))
    # p <- p + theme(legend.text=element_text(size=6, color="black"), legend.position="top")
    p <- p + guides(fill = "none",color ="none",shape ="none")
    
    
```

##### 预测CDS编码区

```shell

transdeconder(){
    transcript_fa=$1 ; out_folder=$2
    LongOrfs=/home/penglingwei/software/miniconda3/bin/TransDecoder.LongOrfs #软件路径
    Predict/home/penglingwei/software/miniconda3/bin/TransDecoder.Predict #软件路径
    Pfam=/home/zhangxiaolian/pengtask/1_predict_transcript/TransDecoder/Pfam/Pfam-A.hmm #数据库路径

    ${LongOrfs} -t ${transcript_fa} -O ${out_folder}
    hmmsearch --cpu 4 -E 1e-10 --domtblout ${out_folder}/pfam.domtblout ${Pfam} ${out_folder}/longest_orfs.pep
    ${Predict} -t ${transcript_fa} -O ${out_folder}
}
```



##### 按单双拆分fastq文件

```
for fq in `ls raw/*/*fastq*`
do
    {
    folder=`dirname ${fq}`
    name=`basename ${fq} | cut -d "." -f 1`
    seqtk seq ${fq} -1 > ${folder}/${name}_R1.fastq
    seqtk seq ${fq} -2 > ${folder}/${name}_R2.fastq
    } &
done
```



##### taxonkit获得物种谱系信息

```
execute() {
  species=$1
  taxid=`echo "${species}" | taxonkit name2taxid | cut -f 2`
  lineage=`echo ${taxid} | taxonkit lineage | taxonkit reformat | cut -f 3`
  echo -e "${species}\t${taxid}\t${lineage}" >> lineage.tsv
}
```



#### NCBI下载sra数据并进行二代RNA-seq基因定量分析

##### 提交脚本示例

```shell
# 00.submit.sh
species="Sus_scrofa"
species="Ovis_aries"

for experiment_accession in `grep -v "^#" input/${species}/experiment_accession.txt`
do
  break
  csub -J plw -q cpu_www -o ${experiment_accession}.out -e ${experiment_accession}.err -n 1 -R span[hosts=1] " \
  sh 01.download_fastq.sh ${experiment_accession} raw/${species}
  "
done

export genome=`ls ref/${species}/*_genomic.fna`
export gtf=`ls ref/${species}/*_genomic.gtf`
# step1：预处理 (只执行一次)
#csub -J plw -q cpu -o log/${sample}.out -e log/${sample}.err -n 48 -R span[hosts=1] "sh 02.ngs.sh preprocess -g ${genome} -f ${gtf} -t 48"
#exit

# step2：
grep -v "^#" input/${species}/raw_data.tsv | while IFS=$'\t' read -r sample fq1 fq2 
do
    break
    csub -J plw -q cpu -o log/${sample}.out -e log/${sample}.err -n 12 -R span[hosts=1] " \
    sh 02.ngs.sh execute -s ${sample} -1 ${fq1} -2 ${fq2} -g ${genome} -f ${gtf} -o output/${species} -t 12 
    "
done

sh 02.ngs.sh matrix -e input/${species}/ORgenes.txt -o output/${species}
```

##### ascp > prefetch的顺序下载sra数据

```shell
# 01.download_fastq.sh
# input
experiment_accession=$1 ; wd=$2
keyfile=/share/home/yzwl_zhouy/software/aspera-3.7.4/etc/asperaweb_id_dsa.openssh

# output
outfolder=${wd}/${experiment_accession} ; mkdir -p ${outfolder}
tsv=${outfolder}/${experiment_accession}.tsv
url="https://www.ebi.ac.uk/ena/portal/api/filereport?accession=${experiment_accession}&result=read_run&fields=run_accession,fastq_md5,fastq_aspera&format=tsv&download=true&limit=0"

echo "=>=>=>${experiment_accession}"
date

echo ">>>step1：从ebi中下载${experiment_accession}的详细信息" 
wget -c ${url} -O ${tsv} 
if [ -s "${tsv}" ] ; then 
  status=1 
else
  echo ">>>无法获得ebi中${experiment_accession}的样本信息" 
  status=0
fi

single () {
  fq=${outfolder}/${run}.fastq.gz
  num=0
  while [ "${status}" == 1 ]
  do
    num=$((num+1))
    if [ ${num} -gt 5 ] ; then echo ">>>${run}下载失败, 已多次尝试" ; status=0 ; break ; fi
    if [ ! -f ${fq} ] || [ `md5sum ${fq} | awk '{print $1}'` != "${fastq_md5}" ] ; then
      echo ">>>第${num}次下载" 
      ascp -vQT -l 500M -P33001 -k 1 -i ${keyfile} era-fasp@${fastq_aspera} ${fq}
    else
      echo ">>>ascp成功下载${experiment_accession}数据, 第${num}次成功"
      break
    fi
  done

}

pair () {
  fastq_md51=`echo ${fastq_md5} | awk -F ";" '{print $1}'`
  fastq_md52=`echo ${fastq_md5} | awk -F ";" '{print $2}'`
  fastq_aspera1=`echo ${fastq_aspera} | awk -F ";" '{print $1}'`
  fastq_aspera2=`echo ${fastq_aspera} | awk -F ";" '{print $2}'`
  fq1=${outfolder}/${run}_1.fastq.gz
  fq2=${outfolder}/${run}_2.fastq.gz
  num=0
  while [ "${status}" == 1 ]
  do
    num=$((num+1))
    if [ ${num} -gt 5 ] ; then echo ">>>${run}下载失败, 已多次尝试" ; status=0 ; break ; fi
    if [ ! -f ${fq1} ] || [ ! -f ${fq2} ] || [ `md5sum ${fq1} | awk '{print $1}'` != "${fastq_md51}" ] || [ `md5sum ${fq2} | awk '{print $1}'` != "${fastq_md52}" ] ; then
      echo ">>>第${num}次下载" 
      ascp -vQT -l 500M -P33001 -k 1 -i ${keyfile} era-fasp@${fastq_aspera1} ${fq1}
      ascp -vQT -l 500M -P33001 -k 1 -i ${keyfile} era-fasp@${fastq_aspera2} ${fq2}
    else
      echo ">>>ascp成功下载${experiment_accession}数据, 第${num}次成功"
      break
    fi
  done
}

awk 'NR>1' ${tsv} | while IFS=$'\t' read -r run fastq_md5 fastq_aspera
do
  echo ">>>step2：检测fastq_aspera是否有效" 
  if [ "${fastq_aspera}" == "" ] ; then echo ">>>fastq_aspera无效" ; status=0 ; fi
  end=`echo ${fastq_aspera} | awk -F ";" '{print NF}'`
  if [ "${end}" == 1 ] ; then
    single
  else
    pair
  fi
done

if [ "${status}" == 0 ] ; then
  echo ">>>step2：prefetch开始下载数据" 
  prefetch ${experiment_accession} -O ${outfolder} && fastq-dump --split-3 `ls ${outfolder}/*/*sra` -O ${outfolder} && echo ">>>prefetch成功下载${experiment_accession}数据"
fi
date
```

##### 二代RNA-seq流程

```shell
# 02.ngs.sh
# module
preprocess () {
    hisat2-build -p ${thread} ${genome} ${genome}
    grep -P '\btranscript_id\s+"[^"]+"' ${gtf} | grep -v '; transcript_id "unknown_transcript_' > ${gtf}.fixed
}

quality_control () {
    output=${out_folder}/quality_control ; mkdir -p ${output}
    qc_fq1=${output}/${sample}_R1.fq
    qc_fq2=${output}/${sample}_R2.fq
    #fastp -i ${raw_fq1} -o ${qc_fq1} -I ${raw_fq2} -O ${qc_fq2} -j ${output}/${sample}.json -h ${output}/${sample}.josn
}

genome_mapping () {
    output=${out_folder}/genome_mapping ; mkdir -p ${output}
    bam=${output}/${sample}.sorted.bam
    hisat2 -p ${thread} --dta -t -x ${genome} -1 ${qc_fq1} -2 ${qc_fq2} | samtools view -@ ${thread} -bS | samtools sort -@ ${thread} -o ${bam}
}

gene_quant () {
    output=${out_folder}/gene_quant/${sample} ; mkdir -p ${output}
    stringtie ${bam} -G ${gtf}.fixed -p ${thread} -A ${output}/gene_expression.tsv -B -C ${output}/transcript_coverage.tsv -e -o ${output}/transcript.gtf
}


matrix () {
python3 - << END
import os
import pandas as pd
import os

genes, out_folder = os.environ["genes"], os.environ["out_folder"]
folder = f"{out_folder}/gene_quant"
genes = pd.read_table(genes, header=None).loc[:, 0].tolist() 
dfs = pd.DataFrame({"gene": []})
for sample in os.listdir(folder):
   df = pd.read_table(f"{folder}/{sample}/gene_expression.tsv", header=0, sep="\t").loc[:, ["Gene ID", "TPM"]]
   df.columns = ["gene", sample]
   df = df[df["gene"].isin(genes)]
   dfs = pd.merge(dfs, df, how="outer", on="gene")
dfs.fillna(0).to_csv(f"{out_folder}/gene_expression.TPM.tsv", header=True, index=False, sep="\t")

END
}


subcommand=$1
shift 1

# parameter
while getopts "s:1:2:g:f:e:t:o:" opt
do
case $opt in
    s) sample=$OPTARG;;
    1) raw_fq1=$OPTARG;;
    2) raw_fq2=$OPTARG;;
    g) genome=$OPTARG;;
    f) gtf=$OPTARG;;
    e) genes=$OPTARG;;
    t) thread=$OPTARG;;
    o) out_folder=$OPTARG;;
    \?) echo "Invalid option: -$OPTARG" >&2; exit 1;;
esac
done

# subcommand
case ${subcommand} in 
    preprocess)
        preprocess
        ;;
    execute)
        quality_control
        genome_mapping
        gene_quant
        ;;
    matrix)
        export genes=${genes}
        export out_folder=${out_folder}
        matrix
        ;;
esac
```

##### 合并cattleGTEX数据爬虫下载的表达数据

```python
import pandas as pd
import os
dfs = pd.DataFrame()
for tsv in os.listdir("raw"):
    gene = tsv.split(".")[0]
    tsv = f"raw/{tsv}"
    df = pd.read_table(tsv, header=0, sep="\t")
    df = df.loc[1:, ["Sample", "TPM"]].set_index("Sample").rename(columns={"TPM": gene}).transpose() # tsv第二行没有数据
    dfs = pd.concat([dfs, df], axis=0)
dfs.to_csv("gene_expression.tsv", header=True, index=True, sep="\t")
```



##### Linux切换中英文

```
LANG="en_US.UTF-8"
LANG="zh_CN.UTF-8"
```



##### Linux大小写

```shell
${parameter^pattern} # 首字母大写
${parameter^^pattern} # 全部大写
${parameter,pattern} # 首字母小写
${parameter,,pattern} # 全部小写

```

